{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Linear Algebra**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***Vector***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is a Vector\n",
    ">A vector is a tuple of one or more values called `scalars`标量.\n",
    "\n",
    "Vectors are often represented using a lowercase character such as “v”;\n",
    "\n",
    "```python\n",
    "v = (v1, v2, v3)\n",
    "```\n",
    "Where v1, v2, v3 are scalar values, often real values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining a Vector\n",
    "\n",
    ">We can represent a vector in Python as a `NumPy array`.\n",
    "\n",
    "A NumPy array can be created from a list of numbers. For example, below we define a vector with the length of 3 and the integer values 1, 2, and 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-17T07:21:58.233512Z",
     "start_time": "2019-12-17T07:21:58.223512Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3]\n"
     ]
    }
   ],
   "source": [
    "from numpy import array\n",
    "v = array([1, 2, 3])\n",
    "print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### vector multiplication\n",
    "$ c = a * b $\n",
    "\n",
    ">As with addition and subtraction, this operation is performed `element-wise` to result in a new vector of the same length.\n",
    "\n",
    "$ a * b = (a_1 * b_1, a_2 * b_2, a_3 * b_3) $   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-17T07:47:44.605654Z",
     "start_time": "2019-12-17T07:47:44.594659Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3]\n",
      "[1 2 3]\n",
      "[1 4 9]\n"
     ]
    }
   ],
   "source": [
    "# multiply vectors\n",
    "from numpy import array\n",
    "a = array([1, 2, 3])\n",
    "print(a)\n",
    "b = array([1, 2, 3])\n",
    "print(b)\n",
    "c = a * b\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-17T07:47:49.433114Z",
     "start_time": "2019-12-17T07:47:49.424111Z"
    }
   },
   "source": [
    "### Vector dot product  `v1.dot(v2)`\n",
    ">We can calculate the sum of the multiplied elements of two vectors of the same length to give a scalar.\n",
    "\n",
    "The dot product is the key tool for calculating `vector projections`, `vector decompositions`, and `determining orthogonality`. The name dot product comes from the symbol used to denote it.\n",
    "\n",
    "$ c = (a_1b_1 + a_2b_2 + a_3b_3) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-17T07:48:15.722568Z",
     "start_time": "2019-12-17T07:48:15.711563Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 4 6]\n",
      "[0 0 0]\n",
      "[1. 1. 1.]\n",
      "14\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(\n",
    "    a + b,\n",
    "    a - b,\n",
    "    a / b,\n",
    "    a.dot(b),\n",
    "    sep='\\n'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***Matrices***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is a Matrix?\n",
    "\n",
    ">A matrix is a ***two-dimensional array of scalars*** with one or more columns and one or more rows.\n",
    "\n",
    "The notation for a matrix is often an `uppercase letter`, such as A, and entries are referred to by their two-dimensional `subscript`下标 of row (i) and column (j), such as $a_{ij}$.    \n",
    "For example: $ A = ((a_{11}, a_{12}), (a_{21}, a_{22}), (a_{31}, a_{32})) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining a Matrix\n",
    "\n",
    ">We can represent a matrix in Python using a two-dimensional NumPy array.\n",
    "\n",
    "A NumPy array can be constructed given ***a list of lists***."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-17T08:09:59.676873Z",
     "start_time": "2019-12-17T08:09:59.668868Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3]\n",
      " [4 5 6]]\n"
     ]
    }
   ],
   "source": [
    "# create matrix\n",
    "from numpy import array\n",
    "A = array([[1, 2, 3], [4, 5, 6]])\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix Addition\n",
    ">Two matrices with the same dimensions can be added together to create a new third matrix.\n",
    "\n",
    "`C = A + B`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-17T08:19:38.907709Z",
     "start_time": "2019-12-17T08:19:38.894704Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3]\n",
      " [4 5 6]]\n",
      "[[1 2 3]\n",
      " [4 5 6]]\n",
      "[[ 2  4  6]\n",
      " [ 8 10 12]]\n"
     ]
    }
   ],
   "source": [
    "# add matrices\n",
    "from numpy import array\n",
    "A = array([[1, 2, 3], [4, 5, 6]])\n",
    "print(A)\n",
    "B = array([[1, 2, 3], [4, 5, 6]])\n",
    "print(B)\n",
    "C = A + B\n",
    "print(C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix Dot Product  `A.dot(B)`\n",
    "\n",
    "Matrix multiplication, also called the matrix dot product, is more complicated than the previous operations and involves a rule, as not all matrices can be multiplied together.\n",
    "\n",
    "$ C = A * B $\n",
    "\n",
    "The rule for matrix multiplication is as follows: \n",
    ">The number of columns in the first matrix (A) must equal the number of rows in the second matrix (B).\n",
    "\n",
    "`C(m,k) = A(m,n) * B(n,k)`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-17T09:33:43.879751Z",
     "start_time": "2019-12-17T09:33:43.861750Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2]\n",
      " [3 4]\n",
      " [5 6]]\n",
      "[[1 2]\n",
      " [3 4]]\n",
      "[[ 7 10]\n",
      " [15 22]\n",
      " [23 34]]\n",
      "[[24]\n",
      " [54]\n",
      " [84]]\n"
     ]
    }
   ],
   "source": [
    "# matrix dot product\n",
    "from numpy import array\n",
    "A = array([[1, 2], [3, 4], [5, 6]])\n",
    "print(A)\n",
    "B = array([[1, 2], [3, 4]])\n",
    "print(B)\n",
    "C = A.dot(B)  # or use @ operator for dot product, C = A @ B\n",
    "print(C)\n",
    "v = array([[6], [9]])\n",
    "print(A.dot(v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-17T08:47:35.485363Z",
     "start_time": "2019-12-17T08:47:35.456364Z"
    }
   },
   "source": [
    "### Hadamard product 阿达马积\n",
    ">For two matrices A and B of the same dimension m × n 维度相同, the Hadamard product A ∘ B (or A ⊙ B) is a matrix of the same dimension as the operands, with elements given by    \n",
    "$(A\\circ B)_{ij}=(A\\odot B)_{ij}=(A)_{ij}(B)_{ij}$\n",
    "\n",
    "For matrices of different dimensions (m × n and p × q, where m ≠ p or n ≠ q) the Hadamard product is undefined.\n",
    "\n",
    "The Hadamard product is also often denoted using the ⊙ or ∘."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***Matrix Types and Operations*** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ?? Types\n",
    "\n",
    "$$ Identity Matrix = \\begin{bmatrix}1&0&0\\\\0&1&0\\\\0&0&1\\end{bmatrix} $$\n",
    "$$ Orthogonal Matrix  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transpose  `A.T`\n",
    ">A defined matrix can be transposed, which creates a new matrix with the number of columns and rows flipped.\n",
    "\n",
    "$ C = A^{T} $\n",
    "\n",
    "We can transpose a matrix in NumPy by calling the `T` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-17T09:39:28.551163Z",
     "start_time": "2019-12-17T09:39:28.541161Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2]\n",
      " [3 4]\n",
      " [5 6]]\n",
      "[[1 3 5]\n",
      " [2 4 6]]\n"
     ]
    }
   ],
   "source": [
    "# transpose matrix\n",
    "from numpy import array\n",
    "A = array([[1, 2], [3, 4], [5, 6]])\n",
    "print(A)\n",
    "C = A.T\n",
    "print(C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ***?? Inversion  `np.linalg.inv(A)`***\n",
    ">Matrix inversion is a process that finds another matrix that when multiplied with the matrix, results in an `identity matrix`. Given a matrix A, find matrix B, such that $ AB = I^n $ or $ BA = I^n $.\n",
    "\n",
    "$$ B = A^{-1} $$\n",
    "\n",
    "The matrix inversion operation is not computed directly, but rather the inverted matrix is\n",
    "discovered through a numerical operation, where a suite of efficient methods may be used, often involving forms of **`matrix decomposition`矩阵分解**.    \n",
    "A matrix can be inverted in NumPy using the `inv()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-18T02:50:38.958773Z",
     "start_time": "2019-12-18T02:50:38.897778Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 2.]\n",
      " [3. 4.]]\n",
      "[[-2.   1. ]\n",
      " [ 1.5 -0.5]]\n",
      "[[1.00000000e+00 1.11022302e-16]\n",
      " [0.00000000e+00 1.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "# invert matrix\n",
    "from numpy import array\n",
    "from numpy.linalg import inv\n",
    "# define matrix\n",
    "A = array([[1.0, 2.0], [3.0, 4.0]])\n",
    "print(A)\n",
    "# invert matrix\n",
    "B = inv(A)\n",
    "print(B)\n",
    "I = A.dot(B)\n",
    "print(I)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trace  `np.tracne(A)`\n",
    ">A trace of a `square matrix` is the sum of the values on the main diagonal of the matrix (top-left to bottom-right)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-18T02:54:28.914301Z",
     "start_time": "2019-12-18T02:54:28.903302Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3]\n",
      " [4 5 6]\n",
      " [7 8 9]]\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "# matrix trace\n",
    "from numpy import array\n",
    "from numpy import trace\n",
    "# define matrix\n",
    "A = array([\n",
    "    [1, 2, 3],\n",
    "    [4, 5, 6],\n",
    "    [7, 8, 9]])\n",
    "print(A)\n",
    "# calculate trace\n",
    "B = trace(A)\n",
    "print(B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ***?? Determinant 矩阵行列式  `np.linalg.det(A)`***\n",
    "\n",
    "The determinant of a `square matrix` is a scalar representation of the volume of the matrix.\n",
    "    \n",
    "    　　The determinant describes the relative geometry of the vectors that make up the　rows of the matrix. More specifically, the determinant of a matrix A tells you the　volume of a box with sides given by rows of A.\n",
    "                                  Page 119, No Bullshit Guide To Linear Algebra, 2017.\n",
    "                                  \n",
    "It is denoted by the $det(A)$ notation or $\\left|A\\right|$ , where A is the matrix on which we are calculating the determinant.\n",
    ">　　The determinant of a square matrix is calculated from the elements of the matrix. More\n",
    "technically, ***the determinant is the product of all the `eigenvalues`特征值 of the matrix.***    \n",
    "　　Eigenvalues are introduced in the lessons on **`matrix factorization`**矩阵因子分解.    \n",
    "　　The intuition for the determinant is that it describes the way a matrix will scale another matrix when they are multiplied together.    \n",
    "\n",
    "In NumPy, the determinant of a matrix can be calculated using the `det()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-18T03:13:40.134100Z",
     "start_time": "2019-12-18T03:13:40.121094Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3]\n",
      " [4 5 6]\n",
      " [7 8 9]]\n",
      "-9.51619735392994e-16\n"
     ]
    }
   ],
   "source": [
    "# matrix determinant\n",
    "from numpy import array\n",
    "from numpy.linalg import det\n",
    "# define matrix\n",
    "A = array([\n",
    "    [1, 2, 3],\n",
    "    [4, 5, 6],\n",
    "    [7, 8, 9]])\n",
    "print(A)\n",
    "# calculate determinant\n",
    "B = det(A)\n",
    "print(B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ***?? Rank  `np.linalg.matrix_rank(A)`***\n",
    "\n",
    ">The rank of a matrix is the estimate of the number of ***`linearly independent`$^{[1]}$*** rows or columns in\n",
    "a matrix.    \n",
    "\n",
    "The rank of a matrix M is often denoted as the function rank().\n",
    "\n",
    "$$rank(A)$$\n",
    "\n",
    "An intuition for rank is to consider it the number of `dimensions spanned`跨纬度 by all of the vectors within a matrix.\n",
    "\n",
    "In the theory of vector spaces 向量空间, a set of vectors is said to be linearly dependent if at least one of the vectors in the set can be defined as a linear combination of the others; if no vector in the set can be written in this way, then the vectors are said to be linearly independent.\n",
    "\n",
    "    　　Linear Independent [1]: In linear algebra, the rank of a matrix A is the dimension of the vector space generated (or spanned) by its columns. This corresponds to the maximal number of linearly independent columns of A. This, in turn, is identical to the dimension of the vector space spanned by its rows. \n",
    "    　　Rank is thus a measure of the \"nondegenerateness\" of the system of linear equations and linear transformation encoded by A.\n",
    "                                                         --- wikipedia\n",
    "\n",
    "For example, a rank of 0 suggest all vectors span a point, a rank of 1 suggests all vectors span a line, a rank of 2 suggests all vectors span a two-dimensional plane.\n",
    "\n",
    ">The rank is estimated numerically, often using a ***`matrix decomposition`*** method. A common approach is to use the ***`Singular-Value Decomposition`*** or `SVD` for short.    \n",
    "\n",
    "NumPy provides the matrix `rank()` function for calculating the rank of an array.\n",
    "\n",
    "矩阵行是观测，列是变量，线性独立可以看作观测间的独立和变量间的独立。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-18T06:00:46.192993Z",
     "start_time": "2019-12-18T06:00:46.178987Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3]\n",
      "1\n",
      "[0 0 0 0 0]\n",
      "0\n",
      "[[1 2 3 4]\n",
      " [3 7 4 7]\n",
      " [1 2 3 4]]\n",
      "matrix ma rank: 2\n",
      "[[1 2 3 4]\n",
      " [3 7 4 7]\n",
      " [2 4 6 8]]\n",
      "matrix ma1 rank: 2\n"
     ]
    }
   ],
   "source": [
    "# vector rank\n",
    "from numpy import array\n",
    "from numpy.linalg import matrix_rank\n",
    "# rank\n",
    "v1 = array([1,2,3])\n",
    "print(v1)\n",
    "vr1 = matrix_rank(v1)\n",
    "print(vr1)\n",
    "# zero rank\n",
    "v2 = array([0,0,0,0,0])\n",
    "print(v2)\n",
    "vr2 = matrix_rank(v2)\n",
    "print(vr2)\n",
    "ma = array([[1,2,3,4], [3,7,4,7], [1,2,3,4]])\n",
    "ma1 = array([[1,2,3,4], [3,7,4,7], [2,4,6,8]])\n",
    "print(\n",
    "    ma, 'matrix ma rank: {}'.format(matrix_rank(ma)),\n",
    "    ma1, 'matrix ma1 rank: {}'.format(matrix_rank(ma1)),\n",
    "    sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-18T05:58:19.710985Z",
     "start_time": "2019-12-18T05:58:19.705982Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***Matrix Factorization/Matrix Decomposition 矩阵因子分解***  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is a Matrix Decomposition?\n",
    "\n",
    ">　　A matrix decomposition is a way of reducing a matrix into its constituent parts.    \n",
    "　　It is an approach that can simplify more complex matrix operations that can be performed on the decomposed matrix rather than on the original matrix itself.\n",
    "  \n",
    "A common analogy for matrix decomposition is the factoring 因式分解 of numbers, such as the factoring of 25 into 5 x 5. For this reason, matrix decomposition is also called matrix factorization. Like factoring real values, there are many ways to decompose a matrix, hence there are a range of different matrix decomposition techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LU Matrix Decomposition  `scipy.linalg.lu(A)`\n",
    "\n",
    ">The LU decomposition is for `square matrices` and decomposes a matrix into L and U components.\n",
    "\n",
    "$$ A = L \\cdot U $$\n",
    "\n",
    "Where A is the square matrix that we wish to decompose, L is the lower triangle matrix, and U is the upper triangle matrix. A variation of this decomposition that is numerically more stable to solve in practice is called the LUP decomposition, or the LU decomposition with ***`partial pivoting`$^{[1]}$***.\n",
    "\n",
    "$$ A = P \\cdot L \\cdot U $$\n",
    "\n",
    "The rows of the parent matrix are re-ordered to simplify the decomposition process and the additional P matrix specifies a way to permute重新排列 the result or return the result to the original order. There are also other variations of the LU.\n",
    "\n",
    "    　　Partial pivoting [1]:In partial pivoting, the algorithm selects the entry with largest absolute value from the column of the matrix that is currently being considered as the pivot element.\n",
    "       Partial pivoting is generally sufficient to adequately reduce round-off error. However, for certain systems and algorithms, complete pivoting (or maximal pivoting) may be required for acceptable accuracy. Complete pivoting interchanges交换 both rows and columns in order to use the largest (by absolute value) element in the matrix as the pivot. \n",
    "       Complete pivoting is usually not necessary to ensure numerical stability and, due to the additional cost of searching for the maximal element, the improvement in numerical stability that it provides is typically outweighed by its reduced efficiency for all but the smallest matrices. Hence, it is rarely used.\n",
    "                                                          --- wikipedia \n",
    "[wikipedia](https://en.wikipedia.org/wiki/Pivot_element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-18T09:05:08.030759Z",
     "start_time": "2019-12-18T09:05:07.666761Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3]\n",
      " [4 5 6]\n",
      " [7 8 9]]\n",
      "[[0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]]\n",
      "[[1.         0.         0.        ]\n",
      " [0.14285714 1.         0.        ]\n",
      " [0.57142857 0.5        1.        ]]\n",
      "[[ 7.00000000e+00  8.00000000e+00  9.00000000e+00]\n",
      " [ 0.00000000e+00  8.57142857e-01  1.71428571e+00]\n",
      " [ 0.00000000e+00  0.00000000e+00 -1.58603289e-16]]\n",
      "[[1. 2. 3.]\n",
      " [4. 5. 6.]\n",
      " [7. 8. 9.]]\n"
     ]
    }
   ],
   "source": [
    "# LU decomposition\n",
    "from numpy import array\n",
    "from scipy.linalg import lu\n",
    "# define a square matrix\n",
    "A = array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "print(A)\n",
    "# LU decomposition\n",
    "P, L, U = lu(A)\n",
    "print(P)\n",
    "print(L)\n",
    "print(U)\n",
    "# reconstruct\n",
    "B = P.dot(L).dot(U)\n",
    "print(B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***QR Decomposition  `np.linalg.qr(A)`***\n",
    "\n",
    ">The QR decomposition is for `m × n` matrices (not limited to square matrices) and decomposes a matrix into Q and R components.\n",
    "\n",
    "$$ A = Q \\cdot R $$\n",
    "\n",
    "Where A is the matrix that we wish to decompose, Q a matrix with the size `m × m`, and R is an upper triangle matrix with the size `m × n`. The QR decomposition is found using an `iterative numerical method` that can fail for those matrices that cannot be decomposed, or decomposed easily.\n",
    "\n",
    "The QR decomposition can be implemented in NumPy using the `qr()` function. By default,\n",
    "the function returns the Q and R matrices with smaller or reduced dimensions that is more economical.     \n",
    "We can change this to return the expected sizes of m × m for Q and m × n for\n",
    "R by specifying the mode argument as `‘complete’`, although this is not required for most\n",
    "applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-18T09:47:31.083304Z",
     "start_time": "2019-12-18T09:47:31.052298Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2]\n",
      " [3 4]\n",
      " [5 6]]\n",
      "[[-0.16903085  0.89708523  0.40824829]\n",
      " [-0.50709255  0.27602622 -0.81649658]\n",
      " [-0.84515425 -0.34503278  0.40824829]]\n",
      "[[-5.91607978 -7.43735744]\n",
      " [ 0.          0.82807867]\n",
      " [ 0.          0.        ]]\n",
      "[[1. 2.]\n",
      " [3. 4.]\n",
      " [5. 6.]]\n"
     ]
    }
   ],
   "source": [
    "# QR decomposition\n",
    "from numpy import array\n",
    "from numpy.linalg import qr\n",
    "# define rectangular matrix\n",
    "A = array([\n",
    "[1, 2],\n",
    "[3, 4],\n",
    "[5, 6]])\n",
    "print(A)\n",
    "# factorize\n",
    "Q, R = qr(A, 'complete')\n",
    "print(Q)\n",
    "print(R)\n",
    "# reconstruct\n",
    "B = Q.dot(R)\n",
    "print(B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cholesky Decomposition  `np.linalg.cholesky(A)`\n",
    ">The Cholesky decomposition is for ***`square symmetric matrices`*** where all values are greater than zero, so-called `positive definite matrices`.    \n",
    "For our interests in machine learning, we will focus on the Cholesky decomposition for real-valued matrices and ignore the cases when working with complex numbers.\n",
    "\n",
    "$$ A = L \\cdot L^{T} $$\n",
    "\n",
    "Where A is the matrix being decomposed, L is the lower triangular matrix and LT is the\n",
    "transpose of L.\n",
    "\n",
    "$$ A = U^{T} \\cdot U $$\n",
    "\n",
    "Where U is the upper triangular matrix. \n",
    "\n",
    "The Cholesky decomposition is used for solving `linear least squares` for linear regression, as well as simulation and optimization methods.    \n",
    "When decomposing symmetric matrices, the Cholesky decomposition is nearly twice as efficient as the LU decomposition and should be preferred in these cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-18T10:10:44.513146Z",
     "start_time": "2019-12-18T10:10:44.476148Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2 1 1]\n",
      " [1 2 1]\n",
      " [1 1 2]]\n",
      "[[1.41421356 0.         0.        ]\n",
      " [0.70710678 1.22474487 0.        ]\n",
      " [0.70710678 0.40824829 1.15470054]]\n",
      "[[2. 1. 1.]\n",
      " [1. 2. 1.]\n",
      " [1. 1. 2.]]\n"
     ]
    }
   ],
   "source": [
    "# Cholesky decomposition\n",
    "from numpy import array\n",
    "from numpy.linalg import cholesky\n",
    "# define symmetrical matrix\n",
    "A = array([\n",
    "[2, 1, 1],\n",
    "[1, 2, 1],\n",
    "[1, 1, 2]])\n",
    "print(A)\n",
    "# factorize\n",
    "L = cholesky(A)\n",
    "print(L)\n",
    "# reconstruct\n",
    "B = L.dot(L.T)\n",
    "print(B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***Eigendecomposition***\n",
    "\n",
    ">　　The most used type of matrix decomposition is the eigendecomposition that decomposes a matrix into `eigenvectors` and `eigenvalues`.    \n",
    "　　Matrix decompositions are a useful tool for reducing a matrix to their constituent parts in order to simplify a range of more complex operations.    \n",
    "　　Eigendecomposition of a matrix is a type of decomposition that involves decomposing a ***`square matrix`*** into ***a set of eigenvectors and eigenvalues***.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eigendecomposition of a Matrix\n",
    ">　　Eigendecomposition of a matrix is a type of decomposition that involves decomposing a `square matrix` into ***a set of eigenvectors and eigenvalues***.    \n",
    "　　Eigendecomposition is used as an element to simplify the calculation of other more complex matrix operations.\n",
    "\n",
    "A vector is an eigenvector of a matrix if it satisfies the following equation.\n",
    "\n",
    "$$ A \\cdot v = \\lambda \\cdot v $$\n",
    "\n",
    "This is called the eigenvalue equation, where A is the parent square matrix that we are decomposing, `v` is the `eigenvector of the matrix`, and `λ` is the lowercase Greek letter lambda and represents the `eigenvalue scalar`.\n",
    "\n",
    "A matrix could have one eigenvector and eigenvalue for each dimension of the parent matrix. Not all square matrices can be decomposed into eigenvectors and eigenvalues, and some can only be decomposed in a way that requires complex numbers. The parent matrix can be shown to be a product of the eigenvectors and eigenvalues.\n",
    "\n",
    "$$ A = Q \\cdot \\Lambda \\cdot Q^T $$\n",
    "\n",
    "Where `Q` is a `matrix comprised of the eigenvectors`, `Λ` is the uppercase Greek letter lambda and is the `diagonal matrix comprised of the eigenvalues`, and QT is the transpose of the matrix comprised of the eigenvectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eigenvectors and Eigenvalues\n",
    "\n",
    ">　　`Eigenvectors` are unit vectors, which means that their length or magnitude is equal to 1.0. They are often referred as right vectors, which simply means a `column vector` (as opposed to a row vector or a left vector).    \n",
    "　　`Eigenvalues` are `coefficients` applied to eigenvectors that give the vectors their length or magnitude."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculation of Eigendecomposition  `np.linalg.eig()`\n",
    "An eigendecomposition is calculated on a square matrix using an efficient iterative algorithm. Often an eigenvalue is found first, then an eigenvector is\n",
    "found to solve the equation as a set of coefficients.     \n",
    "The eigendecomposition can be calculated in NumPy using the `eig()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-19T02:02:17.350813Z",
     "start_time": "2019-12-19T02:02:17.313818Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3]\n",
      " [4 5 6]\n",
      " [7 8 9]]\n",
      "[ 1.61168440e+01 -1.11684397e+00 -9.75918483e-16]\n",
      "[[-0.23197069 -0.78583024  0.40824829]\n",
      " [-0.52532209 -0.08675134 -0.81649658]\n",
      " [-0.8186735   0.61232756  0.40824829]]\n"
     ]
    }
   ],
   "source": [
    "# eigendecomposition\n",
    "from numpy import array\n",
    "from numpy.linalg import eig\n",
    "# define matrix\n",
    "A = array([\n",
    "[1, 2, 3],\n",
    "[4, 5, 6],\n",
    "[7, 8, 9]])\n",
    "print(A)\n",
    "# factorize\n",
    "values, vectors = eig(A)\n",
    "print(values)\n",
    "print(vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confirm an Eigenvector and Eigenvalue\n",
    "We can confirm that a vector is indeed an eigenvector of a matrix. We do this by multiplying(dot product)\n",
    "the candidate eigenvector, then by the value vector, finally comparing the two results.    \n",
    "The example multiplies the original matrix with the first eigenvector and compares it to the\n",
    "first eigenvector multiplied by the first eigenvalue. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-19T02:12:36.970176Z",
     "start_time": "2019-12-19T02:12:36.954182Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing A.dot(vectors[:, 0]) and vectors[:, 0] * values[0]\n",
      "[ -3.73863537  -8.46653421 -13.19443305]\n",
      "[ -3.73863537  -8.46653421 -13.19443305]\n"
     ]
    }
   ],
   "source": [
    "# confirm eigenvector\n",
    "from numpy import array\n",
    "from numpy.linalg import eig\n",
    "# define matrix\n",
    "A = array([\n",
    "    [1, 2, 3],\n",
    "    [4, 5, 6],\n",
    "    [7, 8, 9]])\n",
    "# factorize\n",
    "values, vectors = eig(A)\n",
    "# confirm first eigenvector\n",
    "print(\"Comparing A.dot(vectors[:, 0]) and vectors[:, 0] * values[0]\")\n",
    "B = A.dot(vectors[:, 0])\n",
    "print(B)\n",
    "C = vectors[:, 0] * values[0]\n",
    "print(C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-19T02:17:15.897368Z",
     "start_time": "2019-12-19T02:17:15.889366Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3]\n",
      " [4 5 6]\n",
      " [7 8 9]]\n",
      "[-0.23197069 -0.52532209 -0.8186735 ]\n"
     ]
    }
   ],
   "source": [
    "print(A, vectors[:, 0], sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-19T02:13:29.246699Z",
     "start_time": "2019-12-19T02:13:29.237701Z"
    }
   },
   "source": [
    "### Reconstruct Matrix\n",
    "+ First, the list of eigenvectors must be taken together as a matrix, where each vector becomes a row. The eigenvalues need to be arranged into a diagonal matrix.\n",
    "+ Next, we need to calculate the inverse of the eigenvector matrix.\n",
    "+ Finally, these elements need to be multiplied together with the dot() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-19T02:17:47.215081Z",
     "start_time": "2019-12-19T02:17:47.193087Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3]\n",
      " [4 5 6]\n",
      " [7 8 9]]\n",
      "[[1. 2. 3.]\n",
      " [4. 5. 6.]\n",
      " [7. 8. 9.]]\n"
     ]
    }
   ],
   "source": [
    "# reconstruct matrix\n",
    "from numpy import diag\n",
    "from numpy.linalg import inv\n",
    "from numpy import array\n",
    "from numpy.linalg import eig\n",
    "# define matrix\n",
    "A = array([\n",
    "    [1, 2, 3],\n",
    "    [4, 5, 6],\n",
    "    [7, 8, 9]])\n",
    "print(A)\n",
    "# factorize\n",
    "values, vectors = eig(A)\n",
    "# create matrix from eigenvectors\n",
    "Q = vectors\n",
    "# create inverse of eigenvectors matrix\n",
    "R = inv(Q)\n",
    "# create diagonal matrix from eigenvalues\n",
    "L = diag(values)\n",
    "# reconstruct the original matrix\n",
    "B = Q.dot(L).dot(R)\n",
    "print(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***Singular-Value Decomposition*** \n",
    ">Perhaps the most known and widely used matrix decomposition method is the `Singular-Value Decomposition`, or `SVD`. All matrices have an SVD, which makes it more stable than other methods, such as the eigendecomposition. As such, it is often used in a wide array of applications including `compressing`, `denoising`, and `data reduction`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the Singular-Value Decomposition\n",
    ">The Singular-Value Decomposition, or SVD for short, is a matrix decomposition method for\n",
    "reducing a matrix to its constituent parts in order to make certain subsequent matrix calculations\n",
    "simpler.\n",
    "\n",
    "$$ A = U \\cdot \\Sigma \\cdot V^T $$\n",
    "\n",
    "Where `A` is the real `n × m matrix` that we wish to decompose, `U` is an `m × m matrix`, `Σ`\n",
    "represented by the uppercase Greek letter sigma) is an `m × n diagonal matrix`, and `V^T` is the V\n",
    "`transpose of an n × n matrix` where T is a superscript. \n",
    "\n",
    "The diagonal values in the `Σ matrix` are known as the `singular values` of the original matrix\n",
    "A. The columns of the `U matrix` are called the `left-singular vectors` of A, and the `columns of V` are called the `right-singular vectors` of A. The SVD is calculated via `iterative numerical\n",
    "methods`.\n",
    "\n",
    "The SVD is used widely both in the calculation of other matrix operations, such as `matrix inverse`, but also as a `data reduction` method in machine learning. SVD can also be used in `least squares linear regression`, `image compression`, and `denoising data`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Singular-Value Decomposition  `np.linalg.svd()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-19T05:37:17.559219Z",
     "start_time": "2019-12-19T05:37:17.434241Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2]\n",
      " [3 4]\n",
      " [5 6]]\n",
      "[[-0.2298477   0.88346102  0.40824829]\n",
      " [-0.52474482  0.24078249 -0.81649658]\n",
      " [-0.81964194 -0.40189603  0.40824829]]\n",
      "[9.52551809 0.51430058]\n",
      "[[-0.61962948 -0.78489445]\n",
      " [-0.78489445  0.61962948]]\n"
     ]
    }
   ],
   "source": [
    "# singular-value decomposition\n",
    "from numpy import array\n",
    "from scipy.linalg import svd\n",
    "# define a matrix\n",
    "A = array([\n",
    "    [1, 2],\n",
    "    [3, 4],\n",
    "    [5, 6]])\n",
    "print(A)\n",
    "# factorize\n",
    "U, s, V = svd(A)\n",
    "print(U)\n",
    "print(s)\n",
    "print(V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reconstruct Matrix\n",
    "The original matrix can be reconstructed from the `U`, `Σ`, and `V^T` elements.    \n",
    "The `U`, `s`, and `V` elements returned from the svd() cannot be multiplied directly.    \n",
    "The `s` vector must be converted into a `diagonal matrix` using the diag() function.    \n",
    "\n",
    "By default, this function will create a square matrix that is m × m, relative to our original matrix. ***This causes a problem as the size of the matrices do not fit the rules of matrix multiplication, where the number of columns in a matrix must match the number of rows in the subsequent matrix.*** After creating the `square Σ diagonal matrix`, the sizes of the matrices are relative to the original n × m matrix that we are decomposing, as follows:\n",
    "\n",
    "$ U(m × m) · Σ(m × m) · V^T (n × n) $   \n",
    "\n",
    "Where, in fact, we require:    \n",
    "\n",
    "$ U(m × m) · Σ(m × n) · V^T (n × n) $\n",
    "\n",
    "***We can achieve this by creating a new `Σ matrix of all zero values` that is `m × n` (e.g. more rows) and populate the first `n × n` part of the matrix with the `square diagonal matrix` calculated via diag().***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-19T07:23:07.682962Z",
     "start_time": "2019-12-19T07:23:07.573959Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2]\n",
      " [3 4]\n",
      " [5 6]]\n",
      "[[1. 2.]\n",
      " [3. 4.]\n",
      " [5. 6.]]\n"
     ]
    }
   ],
   "source": [
    "# reconstruct rectangular matrix from svd\n",
    "from numpy import array\n",
    "from numpy import diag\n",
    "from numpy import zeros\n",
    "from scipy.linalg import svd\n",
    "# define matrix\n",
    "A = array([\n",
    "    [1, 2],\n",
    "    [3, 4],\n",
    "    [5, 6]])\n",
    "print(A)\n",
    "# factorize\n",
    "U, s, V = svd(A)\n",
    "\n",
    "# create m x n Sigma matrix\n",
    "Sigma = zeros((A.shape[0], A.shape[1]))\n",
    "# populate Sigma with n x n diagonal matrix\n",
    "Sigma[:A.shape[1], :A.shape[1]] = diag(s)\n",
    "\n",
    "# reconstruct matrix\n",
    "B = U.dot(Sigma.dot(V))\n",
    "print(B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pseudoinverse  `np.linalg.pinv()`\n",
    ">The pseudoinverse is the generalization of the matrix inverse for square matrices to rectangular matrices where the number of rows and columns are not equal. It is also called the Moore-Penrose Inverse 摩尔－彭若斯广义逆/广义逆.\n",
    "\n",
    "    Matrix inversion is not defined for matrices that are not square. [...] When A has more columns than rows, then solving a linear equation using the pseudoinverse provides one of the many possible solutions.\n",
    "                                     --- Page 46, Deep Learning, 2016.\n",
    "\n",
    "The pseudoinverse is denoted as A+, where A is the matrix that is being inverted and + is a superscript. The pseudoinverse is calculated using the singular value decomposition of A:\n",
    "\n",
    "$$ A^+ = V \\cdot D^+ \\cdot U^T $$    \n",
    "\n",
    "Where `A^+` is the `pseudoinvers`e, `D^+` is the `pseudoinverse of the diagonal matrix Σ` and `V^T` is the `transpose of V^T` . We can get U and V from the `SVD operation`.\n",
    "\n",
    "The `D^+` can be calculated by creating a diagonal matrix from Σ, calculating the `reciprocal of each non-zero element` in `Σ`, and taking the `transpose` if the original matrix was rectangular.\n",
    "\n",
    "$$ \\Sigma = \\begin{bmatrix}S_{1,1}&0&0\\\\0&S_{2,2}&0\\\\0&0&S_{3,3}\\end{bmatrix} $$\n",
    "\n",
    "$$ D^+ = \\begin{bmatrix}\\frac{1}{S_{1,1}}&0&0\\\\0&\\frac{1}{S_{2,2}}&0\\\\0&0&\\frac{1}{S_{3,3}}\\end{bmatrix} $$\n",
    "\n",
    "The pseudoinverse provides one way of solving the `linear regression equation`, specifically when there are more rows than there are columns, which is often the case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-19T08:13:20.868891Z",
     "start_time": "2019-12-19T08:13:20.838869Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.1 0.2]\n",
      " [0.3 0.4]\n",
      " [0.5 0.6]\n",
      " [0.7 0.8]]\n",
      "[[-1.00000000e+01 -5.00000000e+00  9.07607323e-15  5.00000000e+00]\n",
      " [ 8.50000000e+00  4.50000000e+00  5.00000000e-01 -3.50000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "# pseudoinverse\n",
    "from numpy import array\n",
    "from numpy.linalg import pinv\n",
    "# define matrix\n",
    "A = array([\n",
    "    [0.1, 0.2],\n",
    "    [0.3, 0.4],\n",
    "    [0.5, 0.6],\n",
    "    [0.7, 0.8]])\n",
    "print(A)\n",
    "# calculate pseudoinverse\n",
    "B = pinv(A)\n",
    "print(B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can calculate the pseudoinverse manually via the SVD and compare the results to the pinv() function.    \n",
    "+ First we must calculate the SVD.    \n",
    "+ Next we must calculate the reciprocal of each value in the s array.\n",
    "+ Then the s array can be transformed into a diagonal matrix with an added row of zeros to make it rectangular.    \n",
    "+ Finally, we can calculate the pseudoinverse from the elements.\n",
    "\n",
    "$$ A^+ = V^T \\cdot D^T \\cdot U^T $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-19T08:16:15.666280Z",
     "start_time": "2019-12-19T08:16:15.648282Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.1 0.2]\n",
      " [0.3 0.4]\n",
      " [0.5 0.6]\n",
      " [0.7 0.8]]\n",
      "[[-1.00000000e+01 -5.00000000e+00  9.07607323e-15  5.00000000e+00]\n",
      " [ 8.50000000e+00  4.50000000e+00  5.00000000e-01 -3.50000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "# pseudoinverse via svd\n",
    "from numpy import array\n",
    "from numpy.linalg import svd\n",
    "from numpy import zeros\n",
    "from numpy import diag\n",
    "# define matrix\n",
    "A = array([\n",
    "    [0.1, 0.2],\n",
    "    [0.3, 0.4],\n",
    "    [0.5, 0.6],\n",
    "    [0.7, 0.8]])\n",
    "print(A)\n",
    "# factorize\n",
    "U, s, V = svd(A)\n",
    "\n",
    "# reciprocals of s\n",
    "d = 1.0 / s\n",
    "\n",
    "# create m x n D matrix\n",
    "D = zeros(A.shape)\n",
    "# populate D with n x n diagonal matrix\n",
    "D[:A.shape[1], :A.shape[1]] = diag(d)\n",
    "\n",
    "# calculate pseudoinverse\n",
    "B = V.T.dot(D.T).dot(U.T)\n",
    "print(B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensionality Reduction\n",
    ">A popular application of SVD is for dimensionality reduction. Data with a large number of\n",
    "features, such as more features (columns) than observations (rows) may be reduced to a ***smaller subset of features that are most relevant to the prediction problem. The result is a matrix with a lower rank that is said to approximate the original matrix.***    \n",
    "\n",
    "To do this we can perform an SVD operation on the original data and select the top `k` largest `singular values` in `Σ`. These columns can be selected from Σ and the rows selected from V^T .    \n",
    "An approximate `B` of the original matirx `A` can then be reconstructed.\n",
    "\n",
    "$$ B = U \\cdot \\Sigma_k \\cdot V_k^T $$\n",
    "\n",
    ">In natural language processing, this approach can be used on matrices of word occurrences\n",
    "or word frequencies in documents and is called `Latent Semantic Analysis` or `Latent Semantic Indexing`. In practice, we can retain and work with a descriptive subset of the data called `T`. This is a dense summary of the matrix or a projection.\n",
    "\n",
    "$$ T = U \\cdot \\Sigma_k $$\n",
    "\n",
    "Further, this transform can be calculated and applied to the original matrix A as well as\n",
    "other similar matrices.\n",
    "\n",
    "$$ T = A \\cdot V_k^T $$\n",
    "\n",
    "The example below demonstrates data reduction with the SVD. First a 3 × 10 matrix is\n",
    "defined, with more columns than rows. The SVD is calculated and only the first two features are selected. The elements are recombined to give an accurate reproduction of the original matrix. Finally the transform is calculated two different ways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-19T08:38:29.127498Z",
     "start_time": "2019-12-19T08:38:29.103500Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  2  3  4  5  6  7  8  9 10]\n",
      " [11 12 13 14 15 16 17 18 19 20]\n",
      " [21 22 23 24 25 26 27 28 29 30]]\n",
      "[[ 1.  2.  3.  4.  5.  6.  7.  8.  9. 10.]\n",
      " [11. 12. 13. 14. 15. 16. 17. 18. 19. 20.]\n",
      " [21. 22. 23. 24. 25. 26. 27. 28. 29. 30.]]\n",
      "[[-18.52157747   6.47697214]\n",
      " [-49.81310011   1.91182038]\n",
      " [-81.10462276  -2.65333138]]\n",
      "[[-18.52157747   6.47697214]\n",
      " [-49.81310011   1.91182038]\n",
      " [-81.10462276  -2.65333138]]\n"
     ]
    }
   ],
   "source": [
    "# data reduction with svd\n",
    "from numpy import array\n",
    "from numpy import diag\n",
    "from numpy import zeros\n",
    "from scipy.linalg import svd\n",
    "# define matrix\n",
    "A = array([\n",
    "    [1,2,3,4,5,6,7,8,9,10],\n",
    "    [11,12,13,14,15,16,17,18,19,20],\n",
    "    [21,22,23,24,25,26,27,28,29,30]])\n",
    "print(A)\n",
    "# factorize\n",
    "U, s, V = svd(A)\n",
    "\n",
    "# create m x n Sigma matrix\n",
    "Sigma = zeros((A.shape[0], A.shape[1]))\n",
    "# populate Sigma with n x n diagonal matrix\n",
    "Sigma[:A.shape[0], :A.shape[0]] = diag(s)\n",
    "\n",
    "# select\n",
    "n_elements = 2\n",
    "Sigma = Sigma[:, :n_elements]\n",
    "V = V[:n_elements, :]\n",
    "# reconstruct\n",
    "B = U.dot(Sigma.dot(V))\n",
    "print(B)\n",
    "\n",
    "# transform\n",
    "T = U.dot(Sigma)\n",
    "print(T)\n",
    "T = A.dot(V.T)\n",
    "print(T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `scikit-learn` provides a `TruncatedSVD class` that implements this capability directly. The TruncatedSVD class can be created in which you must specify the number of desirable features or components to select, e.g. 2. Once created, you can fit the transform (e.g. calculate $V_k^T$ ) by calling the fit() function, then apply it to the original matrix by calling the transform() function. The result is the transform of A called T above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-19T08:41:22.424664Z",
     "start_time": "2019-12-19T08:41:21.220675Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  2  3  4  5  6  7  8  9 10]\n",
      " [11 12 13 14 15 16 17 18 19 20]\n",
      " [21 22 23 24 25 26 27 28 29 30]]\n",
      "[[18.52157747  6.47697214]\n",
      " [49.81310011  1.91182038]\n",
      " [81.10462276 -2.65333138]]\n"
     ]
    }
   ],
   "source": [
    "# svd data reduction in scikit-learn\n",
    "from numpy import array\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "# define matrix\n",
    "A = array([\n",
    "    [1,2,3,4,5,6,7,8,9,10],\n",
    "    [11,12,13,14,15,16,17,18,19,20],\n",
    "    [21,22,23,24,25,26,27,28,29,30]])\n",
    "print(A)\n",
    "# create transform\n",
    "svd = TruncatedSVD(n_components=2)\n",
    "# fit transform\n",
    "svd.fit(A)\n",
    "# apply transform\n",
    "result = svd.transform(A)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Statistics**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Multivariate Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected Value and Mean\n",
    "***`In probability`***, the average value of some random variable X is called the expected value or the expectation.\n",
    "It is calculated as the `probability weighted sum of values` that can be drawn.\n",
    "\n",
    "$$ E[x] = \\sum x_1p_1, x_2p_2, x_3p_3, \\cdots, x_np_n  $$\n",
    "\n",
    "In simple cases, such as the flipping of a coin or rolling a dice, the probability of each event is just as likely. Therefore, the expected value can be calculated as the sum of all values multiplied by the reciprocal of the number of values.\n",
    "\n",
    "$$ E[x] = \\frac{1}{n} \\times \\sum x_1, x_2, x_3, \\cdots, x_n $$\n",
    "\n",
    "---\n",
    "\n",
    "***`In statistics`***, the mean, or more technically the arithmetic mean or sample mean, can be estimated from a sample of examples drawn from the domain. It is confusing because mean, average, and expected value are used interchangeably. In the abstract, the mean is denoted by the lower case Greek letter mu `µ` and is `calculated from the sample of observations`, rather than all possible values.\n",
    "\n",
    "$$ \\mu = \\frac{1}{n} \\times \\sum x_1, x_2, x_3, \\cdots, x_n $$\n",
    "or\n",
    "$$ \\mu = P(x) \\times \\sum x $$\n",
    "\n",
    "Where x is the vector of observations and P(x) is the calculated probability for each value.When calculated for a specific variable, such as x, the mean is denoted as a lower case variable name with a line above, called x-bar e.g. $\\bar{x}$.\n",
    "\n",
    "$$ \\bar{x} = \\frac{1}{n} \\times \\sum_{i=1}^{n} x_i $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-19T09:50:27.288521Z",
     "start_time": "2019-12-19T09:50:27.261541Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3 4 5 6]\n",
      " [1 2 3 4 5 6]]\n",
      "[1. 2. 3. 4. 5. 6.]\n",
      "[3.5 3.5]\n"
     ]
    }
   ],
   "source": [
    "# matrix means\n",
    "from numpy import array\n",
    "from numpy import mean\n",
    "# define matrix\n",
    "M = array([\n",
    "    [1,2,3,4,5,6],\n",
    "    [1,2,3,4,5,6]])\n",
    "print(M)\n",
    "# column means\n",
    "col_mean = mean(M, axis=0)\n",
    "print(col_mean)\n",
    "# row means\n",
    "row_mean = mean(M, axis=1)\n",
    "print(row_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variance and Standard Deviation\n",
    "\n",
    "`In probability`, covariance is the measure of the joint probability for two random variables. It describes how the two variables change together. It is denoted as the function cov(X; Y ), where X and Y are the two random variables being considered.\n",
    "\n",
    ">Covariance is calculated as expected value or average of the product of the differences of each random variable from their expected values.\n",
    "$$ cov(X, Y) = E[(X - E[X]) \\times (Y - E[Y])] $$\n",
    "\n",
    "Assuming the expected values for X and Y have been calculated, the covariance can be calculated as below:\n",
    "\n",
    "$$ cov(X, Y) = \\frac{1}{n} \\times \\sum_{i=1}^n (x_i - E[X]) \\times (y_i - E[Y]) $$\n",
    "\n",
    "`In statistics`, the sample covariance can be calculated in the same way, although with a bias correction, the same as with the variance.\n",
    "\n",
    "$$ cov(X, Y) = \\frac{1}{n-1} \\times \\sum_{i=1}^n (x_i - E[X]) \\times (y_i - E[Y]) $$\n",
    "\n",
    "The sign of the covariance can be interpreted as whether the two variables increase together (positive) or decrease together (negative).\n",
    "\n",
    "numpy has a function for calculating a covariance matrix called cov() that we can use to retrieve the covariance. By default, the `cov()` function will calculate the `unbiased` or `sample covariance` between the provided random variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-20T03:59:51.818254Z",
     "start_time": "2019-12-20T03:59:51.778237Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3 4 5 6 7 8 9]\n",
      "[9 8 7 6 5 4 3 2 1]\n",
      "-7.5\n"
     ]
    }
   ],
   "source": [
    "# vector covariance\n",
    "from numpy import array\n",
    "from numpy import cov\n",
    "# define first vector\n",
    "x = array([1,2,3,4,5,6,7,8,9])\n",
    "print(x)\n",
    "# define second covariance\n",
    "y = array([9,8,7,6,5,4,3,2,1])\n",
    "print(y)\n",
    "# calculate covariance\n",
    "Sigma = cov(x,y)[0,1]\n",
    "print(Sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The covariance can be normalized to a score between -1 and 1 to make the magnitude interpretable by dividing it by the standard deviation of X and Y . The result is called the correlation of the variables, also called the `Pearson correlation coefficient`.\n",
    "\n",
    "$$ r = \\frac{cov(X, Y)}{s_X s_Y} $$\n",
    "\n",
    "Where `r` is the `correlation coefficient` of X and Y , `cov(X, Y)` is the `sample covariance` of X and Y and `s_X` and `s_Y` are the `standard deviations` of X and Y respectively.\n",
    "\n",
    "As with the results from cov() we can access just the correlation of interest from the [0,1] value from the returned squared matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-20T04:15:15.636157Z",
     "start_time": "2019-12-20T04:15:15.612150Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3 4 5 6 7 8 9]\n",
      "[9 8 7 6 5 4 3 2 1]\n",
      "-1.0\n"
     ]
    }
   ],
   "source": [
    "# vector correlation\n",
    "from numpy import array\n",
    "from numpy import corrcoef\n",
    "# define first vector\n",
    "x = array([1,2,3,4,5,6,7,8,9])\n",
    "print(x)\n",
    "# define second vector\n",
    "y = array([9,8,7,6,5,4,3,2,1])\n",
    "print(y)\n",
    "# calculate correlation\n",
    "corr = corrcoef(x,y)[0,1]\n",
    "print(corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-20T05:49:29.720291Z",
     "start_time": "2019-12-20T05:49:29.711286Z"
    }
   },
   "source": [
    "### ***?? Covariance Matrix***\n",
    ">The covariance matrix is a `square` and `symmetric` matrix that describes the covariance between two or more random variables.    \n",
    "\n",
    "The `diagonal of the covariance matrix` are the `variances` of each of the random variables, as such it is often called the variance-covariance matrix.\n",
    "\n",
    "The covariance matrix is denoted as the uppercase Greek letter Sigma, e.g. `Σ`. The covariance for each pair of random variables is calculated as.\n",
    "\n",
    "$$ \\Sigma = E[(X - E[X]) \\times (Y - E[Y])] \\;where:\\; \\Sigma_{i,j} = cov(X_i, X_j) $$\n",
    "\n",
    "And X is a matrix where each column represents a random variable. The covariance matrix\n",
    "provides a useful tool for separating the structured relationships in a matrix of random variables. This can be used to decorrelate variables or applied as a transform to other variables. \n",
    "\n",
    "The cov() function can be called with a single 2D array where each `sub-array contains a feature` (e.g. column). ***If this function is called with your data defined in a `normal matrix format (rows then columns)`, then a `transpose` of the matrix will need to be provided to the function in order to correctly calculate the covariance of the columns.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-20T07:29:39.381858Z",
     "start_time": "2019-12-20T07:29:39.276855Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  5  8]\n",
      " [ 3  5 11]\n",
      " [ 2  4  9]\n",
      " [ 3  6 10]\n",
      " [ 1  5 10]]\n",
      "[[1.   0.25 0.75]\n",
      " [0.25 0.5  0.25]\n",
      " [0.75 0.25 1.3 ]]\n"
     ]
    }
   ],
   "source": [
    "# covariance matrix\n",
    "from numpy import array\n",
    "from numpy import cov\n",
    "# define matrix of observations\n",
    "X = array([\n",
    "    [1, 5, 8],\n",
    "    [3, 5, 11],\n",
    "    [2, 4, 9],\n",
    "    [3, 6, 10],\n",
    "    [1, 5, 10]])\n",
    "print(X)\n",
    "# calculate covariance matrix\n",
    "Sigma = cov(X.T)\n",
    "print(Sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-20T07:29:44.626952Z",
     "start_time": "2019-12-20T07:29:44.611958Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  3,  2,  3,  1],\n",
       "       [ 5,  5,  4,  6,  5],\n",
       "       [ 8, 11,  9, 10, 10]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Component Analysis\n",
    "An important machine learning method for dimensionality reduction is called Principal Component Analysis. \n",
    "It is a method that uses simple matrix operations from linear algebra and statistics to ***calculate a projection of the original data into the same number or fewer dimensions.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***?? What is Principal Component Analysis***\n",
    ">Principal Component Analysis, or PCA for short, is a method for `reducing the dimensionality of data`. It can be thought of as a projection method where ***data with m-columns (features) is projected into a subspace with m or fewer columns, whilst retaining the essence of the original data.***\n",
    "\n",
    "PCA is an operation applied to a dataset, represented by an n × m matrix A that results in a projection of A which we will call B. Let’s walk through the steps of this operation.\n",
    "\n",
    "$$ A = \\begin{pmatrix} a_{1,1}&a_{1,2} \\\\ a_{2,1}&a_{2,2} \\\\ a_{3,1}&a_{3,2} \\end{pmatrix} $$\n",
    "\n",
    "$$ B = PCA(A) $$\n",
    "\n",
    "1. The first step is to calculate the mean values of each column.\n",
    "\n",
    "$$ M = mean(A) $$\n",
    "\n",
    "2. Next, we need to center the values in each column by subtracting the mean column value.\n",
    "\n",
    "$$ C = A − M $$\n",
    "\n",
    "3. The next step is to calculate the covariance matrix of the centered matrix C. \n",
    "Correlation is a normalized measure of the amount and direction (positive or negative) that two columns change together. Covariance is a generalized and unnormalized version of correlation across multiple columns. A covariance matrix is a calculation of covariance of a given matrix with covariance scores for every column with every other column, including itself.\n",
    "\n",
    "$$ V = cov(C) $$\n",
    "\n",
    "4. Finally, we calculate the eigendecomposition of the covariance matrix V . This results in a list of eigenvalues and a list of eigenvectors.\n",
    "\n",
    "$$ values; vectors = eig(V) $$ \n",
    "\n",
    "The eigenvectors represent the directions or components for the reduced subspace of B, whereas the eigenvalues represent the magnitudes for the directions. The eigenvectors can be sorted by the eigenvalues in descending order to provide a ranking of the components or axes of the new subspace for A. If all eigenvalues have a similar value, then we know that the existing representation may already be reasonably compressed or dense and that the projection may offer little. If there are eigenvalues close to zero, they represent components or axes of B that may be discarded. A total of m or less components must be selected to comprise the chosen subspace. Ideally, we would select k eigenvectors, called principal components, that have the k largest eigenvalues.\n",
    "\n",
    "$$ B = select(values; vectors) $$\n",
    "\n",
    "Other matrix decomposition methods can be used such as Singular-Value Decomposition, or SVD. As such, generally the values are referred to as singular values and the vectors of the subspace are referred to as principal components. Once chosen, data can be projected into the subspace via matrix multiplication.\n",
    "\n",
    "$$ P = B^T · A $$\n",
    "\n",
    "Where A is the original data that we wish to project, BT is the transpose of the chosen principal components and P is the projection of A. This is called the covariance method for calculating the PCA, although there are alternative ways to calculate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[***Source - machinelearingmastery***](https://machinelearningmastery.com/linear-algebra-machine-learning-7-day-mini-course/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "notify_time": "5",
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
